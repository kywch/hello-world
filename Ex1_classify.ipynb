{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "import csv\n",
    "import os\n",
    "from sklearn import metrics\n",
    "\n",
    "path = 'D:\\Dropbox\\ongoing\\chrisIntervention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['210100_1', '620048_1', '820126_1', '920049_1', '410213_1', '1020134_1', '211904_1', '420055_1', '210306_1', '212100_1', '520085_1', '420037_1', '211002_1', '510621_1', '120002_1', '120017_1', '120064_1', '510109_1', '1120055_1', '320089_1', '212202_1', '920115_1', '211101_1', '320003_1', '110064_1', '910116_1', '1120074_1', '220053_1', '1020069_1', '320038_1', '820094_1', '1110206_1', '510713_1', '710203_1', '520086_1', '920003_1', '420066_1', '520032_1', '420022_1', '710207_1', '1120009_1', '410221_1', '212104_1', '420052_1', '120031_1', '310047_1', '310155_1', '610202_1', '220023_1', '120001_1', '520138_1', '910506_1', '720071_1', '920063_1', '120084_1', '210700_1', '1120126_1', '1020097_1', '520024_1', '1120065_1', '220002_1', '710526_1', '220117_1', '120049_1', '1020101_1', '1120012_1', '810310_1', '710517_1', '110018_1', '510506_1', '520164_1', '920064_1', '1010227_1', '410401_1', '1010225_1', '210401_1', '410122_1', '920085_1', '1110616_1', '520061_1', '610701_1', '1110609_1', '510422_1', '910124_1', '120026_1', '220048_1', '920032_1', '110083_1', '820061_1', '220110_1', '910224_1', '310168_1', '920020_1', '620009_1', '620062_1', '120028_1', '220131_1', '1010219_1', '920055_1', '320078_1', '620114_1', '720017_1', '1010202_1', '1020124_1', '1020135_1', '210105_1', '520057_1', '1110513_1', '110021_1', '710111_1', '710515_1', '110045_1', '310078_1', '210517_1', '120023_1', '220031_1', '910305_1', '920002_1', '510411_1', '420025_1', '910208_1', '620084_1', '1120037_1', '620149_1', '910117_1', '520025_1', '720026_1', '211202_1', '520044_1', '1110111_1', '420057_1', '110049_1', '920029_1', '120039_1', '211005_1', '620002_1', '920004_1', '610414_1', '211212_1', '710417_1', '1020130_1', '210714_1', '1010310_1', '220129_1', '720902_1', '420014_1', '810712_1', '1020030_1', '1110302_1', '620006_1', '1020119_1', '211204_1', '320155_1', '910202_1', '610110_1', '120033_1', '1020042_1', '110070_1', '820075_1', '810903_1', '510718_1', '1120111_1', '320139_1', '1120046_1', '210109_1', '920108_1', '320069_1', '220040_1', '720060_1', '920016_1', '120041_1', '720135_1', '320015_1', '510622_1', '920119_1', '120022_1', '820173_1', '120045_1', '710204_1', '1020106_1', '1120131_1', '211806_1', '220151_1', '920065_1', '320035_1', '1020006_1', '520005_1', '710407_1', '120079_1', '210519_1', '920076_1', '710605_1', '320054_1', '710227_1', '110004_1', '120057_1', '220139_1', '920078_1', '120077_1', '120051_1', '220070_1', '510303_1', '120071_1', '910508_1', '110039_1', '520084_1', '220063_1', '520063_1', '510005_1', '212206_1', '610303_1', '920082_1', '110033_1', '1010150_1', '220089_1', '520056_1', '211105_1', '220019_1', '720028_1', '610521_1', '520205_1', '220116_1', '120067_1', '310031_1', '910410_1', '710416_1', '220072_1', '610210_1', '410411_1', '1120006_1', '1120117_1', '1020118_1', '220120_1', '520049_1', '920109_1', '520095_1', '820211_1', '110017_1', '810407_1', '820056_1', '910316_1', '710619_1', '620130_1', '920015_1', '320098_1', '310095_1', '820009_1', '1010114_1', '710501_1', '1110621_1', '1120035_1', '810204_1', '1020070_1', '1120100_1', '120081_1', '220146_1', '1020054_1', '710608_1', '620045_1', '210818_1', '720053_1', '720059_1', '820018_1', '310011_1', '1010342_1', '110081_1', '210905_1', '110090_1', '520152_1', '910404_1', '620169_1', '920001_1', '820082_1', '810422_1', '620178_1', '720105_1', '820127_1', '1110615_1', '720056_1', '810311_1', '920043_1', '1120149_1', '320085_1', '510402_1', '120016_1', '1110304_1', '1020133_1', '1110116_1', '810811_1', '1020001_1', '110077_1', '320020_1', '120073_1', '210300_1', '210805_1', '620136_1', '1020096_1', '320075_1', '320017_1', '310019_1', '1110519_1', '420064_1', '210121_1', '510717_1', '610114_1', '710110_1', '510615_1', '210405_1', '910417_1', '1010327_1', '620031_1', '1120099_1', '1120054_1', '1020904_1', '320002_1', '710505_1', '820106_1', '510701_1', '910223_1', '110035_1', '1020002_1', '710612_1', '120046_1', '720052_1', '520074_1', '220011_1', '920061_1', '210716_1', '110084_1', '610415_1', '1120133_1', '910318_1', '910121_1', '220168_1', '620008_1', '211007_1', '510721_1', '320037_1', '510603_1', '1020039_1', '1020098_1', '610408_1', '1110117_1', '520102_1', '120086_1'])\n",
      "\n",
      "# of cohort 1:  148\n",
      "# of cohort 2:  197\n"
     ]
    }
   ],
   "source": [
    "# read cohort info from the CSV and save them with the sbj ID\n",
    "\n",
    "intvIdx = 'Ex1'\n",
    "\n",
    "category = {}\n",
    "with open(os.path.join(path, intvIdx + '_key.csv')) as f:\n",
    "    sbjData = csv.reader(f)\n",
    "    next(sbjData) # skip the header\n",
    "    for row in sbjData:\n",
    "        category[row[0]] = int(row[1])\n",
    "\n",
    "subjectId = category.keys()        \n",
    "        \n",
    "print(subjectId)\n",
    "print()\n",
    "print('# of cohort 1: ', sum(1 for sbj in category if category[sbj] == 1))\n",
    "print('# of cohort 2: ', sum(1 for sbj in category if category[sbj] == 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['football', 'more', 'own', 'friends', 'was', 'bad', 'well', 'favorite', 'out', 'say', 'fun', 'help', 'hang', 'much', 'really', 'when', 'mad', 'most', 'times', 'why', 'want', 'art', 'down', 'makes', 'after', 'and', 'can', 'had', 'team', 'new', 'just', 'its', 'happy', 'keep', 'stuff', 'day', 'people', 'spending', 'friend', 'not', 'love', 'anything', 'them', 'because', 'your', 'soccer', 'enjoy', 'someone', 'listening', 'have', 'then', 'also', 'sport', 'the', 'without', 'last', 'great', 'helps', 'these', 'thing', 'things', 'living', 'very', 'home', 'for', 'other', 'sad', 'nice', 'way', 'enjoying', 'second', 'think', 'sports', 'what', 'cause', 'need', 'about', 'important', 'see', 'picked', 'funny', 'this', 'going', 'some', 'don', 'values', 'group', 'only', 'were', 'they', 'take', 'feel', 'should', 'music', 'with', 'something', 'around', 'humor', 'are', 'how', 'thats', 'play', 'lot', 'all', 'time', 'first', 'listen', 'always', 'care', 'draw', 'life', 'through', 'tell', 'independent', 'any', 'together', 'active', 'could', 'dont', 'having', 'but', 'who', 'hanging', 'basketball', 'been', 'playing', 'work', 'alot', 'will', 'get', 'game', 'like', 'sense', 'doing', 'every', 'never', 'live', 'talk', 'now', 'you', 'better', 'sometimes', 'even', 'show', 'there', 'that', 'make', 'school', 'myself', 'bored', 'everything', 'best', 'moment', 'know', 'creative', 'one', 'family', 'being', 'good', 'mom', 'would', 'laugh', 'sing']\n"
     ]
    }
   ],
   "source": [
    "# build the dictionary based on the training set\n",
    "# this is just to check if it is working\n",
    "# we should build the dictionary for each leave-one-out test\n",
    "\n",
    "def make_dictionary(subjectPool, critNumSbj):\n",
    "    allWords = []\n",
    "    for sbj in subjectPool:\n",
    "        sbjWords = []\n",
    "        for line in codecs.open(os.path.join(path, intvIdx, sbj + '.txt'), 'r' , 'utf-8'):\n",
    "            line = line.strip()\n",
    "            line = line.lower()\n",
    "            tokens = nltk.wordpunct_tokenize(line)\n",
    "            text = nltk.Text(tokens)\n",
    "            words = [w for w in text if re.search('[a-z]+',w) and len(w)>2]\n",
    "            sbjWords.extend(words)\n",
    "        # when making dictionary, make the words from a document unique\n",
    "        sbjWords = list(set(sbjWords))\n",
    "        allWords.extend(sbjWords)\n",
    "    \n",
    "    # once we make list of all words, we count the words\n",
    "    cntWords = nltk.FreqDist(allWords)\n",
    "    #for ii in range(21,0,-2):\n",
    "    #    print('# words with freq >', ii, ' : ', sum(1 for word in cntWords if cntWords[word] > ii))\n",
    "    \n",
    "    # we go with the words that came from at least N different people (arbitrary) \n",
    "    # we can change and see what happens ...\n",
    "    return [word for word in cntWords if cntWords[word] > critNumSbj]\n",
    "\n",
    "dictionary = make_dictionary(subjectId, 10)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk output:  ['You', 'think', 'you', 'can', 'do', 'this', 'one', ',', 'Kate', '?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'about': False,\n",
       " 'active': False,\n",
       " 'after': False,\n",
       " 'all': False,\n",
       " 'alot': False,\n",
       " 'also': False,\n",
       " 'always': False,\n",
       " 'and': False,\n",
       " 'any': False,\n",
       " 'anything': False,\n",
       " 'are': False,\n",
       " 'around': False,\n",
       " 'art': False,\n",
       " 'bad': False,\n",
       " 'basketball': False,\n",
       " 'because': False,\n",
       " 'been': False,\n",
       " 'being': False,\n",
       " 'best': False,\n",
       " 'better': False,\n",
       " 'bored': False,\n",
       " 'but': False,\n",
       " 'can': True,\n",
       " 'care': False,\n",
       " 'cause': False,\n",
       " 'could': False,\n",
       " 'creative': False,\n",
       " 'day': False,\n",
       " 'doing': False,\n",
       " 'don': False,\n",
       " 'dont': False,\n",
       " 'down': False,\n",
       " 'draw': False,\n",
       " 'enjoy': False,\n",
       " 'enjoying': False,\n",
       " 'even': False,\n",
       " 'every': False,\n",
       " 'everything': False,\n",
       " 'family': False,\n",
       " 'favorite': False,\n",
       " 'feel': False,\n",
       " 'first': False,\n",
       " 'football': False,\n",
       " 'for': False,\n",
       " 'friend': False,\n",
       " 'friends': False,\n",
       " 'fun': False,\n",
       " 'funny': False,\n",
       " 'game': False,\n",
       " 'get': False,\n",
       " 'going': False,\n",
       " 'good': False,\n",
       " 'great': False,\n",
       " 'group': False,\n",
       " 'had': False,\n",
       " 'hang': False,\n",
       " 'hanging': False,\n",
       " 'happy': False,\n",
       " 'have': False,\n",
       " 'having': False,\n",
       " 'help': False,\n",
       " 'helps': False,\n",
       " 'home': False,\n",
       " 'how': False,\n",
       " 'humor': False,\n",
       " 'important': False,\n",
       " 'independent': False,\n",
       " 'its': False,\n",
       " 'just': False,\n",
       " 'keep': False,\n",
       " 'know': False,\n",
       " 'last': False,\n",
       " 'laugh': False,\n",
       " 'life': False,\n",
       " 'like': False,\n",
       " 'listen': False,\n",
       " 'listening': False,\n",
       " 'live': False,\n",
       " 'living': False,\n",
       " 'lot': False,\n",
       " 'love': False,\n",
       " 'mad': False,\n",
       " 'make': False,\n",
       " 'makes': False,\n",
       " 'mom': False,\n",
       " 'moment': False,\n",
       " 'more': False,\n",
       " 'most': False,\n",
       " 'much': False,\n",
       " 'music': False,\n",
       " 'myself': False,\n",
       " 'need': False,\n",
       " 'never': False,\n",
       " 'new': False,\n",
       " 'nice': False,\n",
       " 'not': False,\n",
       " 'now': False,\n",
       " 'one': True,\n",
       " 'only': False,\n",
       " 'other': False,\n",
       " 'out': False,\n",
       " 'own': False,\n",
       " 'people': False,\n",
       " 'picked': False,\n",
       " 'play': False,\n",
       " 'playing': False,\n",
       " 'really': False,\n",
       " 'sad': False,\n",
       " 'say': False,\n",
       " 'school': False,\n",
       " 'second': False,\n",
       " 'see': False,\n",
       " 'sense': False,\n",
       " 'should': False,\n",
       " 'show': False,\n",
       " 'sing': False,\n",
       " 'soccer': False,\n",
       " 'some': False,\n",
       " 'someone': False,\n",
       " 'something': False,\n",
       " 'sometimes': False,\n",
       " 'spending': False,\n",
       " 'sport': False,\n",
       " 'sports': False,\n",
       " 'stuff': False,\n",
       " 'take': False,\n",
       " 'talk': False,\n",
       " 'team': False,\n",
       " 'tell': False,\n",
       " 'that': False,\n",
       " 'thats': False,\n",
       " 'the': False,\n",
       " 'them': False,\n",
       " 'then': False,\n",
       " 'there': False,\n",
       " 'these': False,\n",
       " 'they': False,\n",
       " 'thing': False,\n",
       " 'things': False,\n",
       " 'think': True,\n",
       " 'this': True,\n",
       " 'through': False,\n",
       " 'time': False,\n",
       " 'times': False,\n",
       " 'together': False,\n",
       " 'values': False,\n",
       " 'very': False,\n",
       " 'want': False,\n",
       " 'was': False,\n",
       " 'way': False,\n",
       " 'well': False,\n",
       " 'were': False,\n",
       " 'what': False,\n",
       " 'when': False,\n",
       " 'who': False,\n",
       " 'why': False,\n",
       " 'will': False,\n",
       " 'with': False,\n",
       " 'without': False,\n",
       " 'work': False,\n",
       " 'would': False,\n",
       " 'you': True,\n",
       " 'your': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define a function that tunrs text into a list of tokens\n",
    "\n",
    "def extract_feature(text, dictionary):\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    text = nltk.Text(tokens)\n",
    "    feature = {}\n",
    "    for word in dictionary:\n",
    "        feature[word] = (word in text)\n",
    "    return feature\n",
    "\n",
    "print('nltk output: ', nltk.wordpunct_tokenize('You think you can do this one, Kate?'))\n",
    "print(extract_feature('You think you can do this one, Kate?', dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: pred - 1, actual - 1\n",
      "2: pred - 2, actual - 2\n",
      "3: pred - 1, actual - 2\n",
      "4: pred - 1, actual - 2\n",
      "5: pred - 1, actual - 1\n",
      "6: pred - 2, actual - 2\n",
      "7: pred - 2, actual - 1\n",
      "8: pred - 2, actual - 2\n",
      "9: pred - 2, actual - 1\n",
      "10: pred - 2, actual - 1\n",
      "11: pred - 2, actual - 2\n",
      "12: pred - 2, actual - 2\n",
      "13: pred - 1, actual - 1\n",
      "14: pred - 2, actual - 1\n",
      "15: pred - 2, actual - 2\n",
      "16: pred - 2, actual - 2\n",
      "17: pred - 2, actual - 2\n",
      "18: pred - 1, actual - 1\n",
      "19: pred - 2, actual - 2\n",
      "20: pred - 2, actual - 2\n",
      "21: pred - 2, actual - 1\n",
      "22: pred - 2, actual - 2\n",
      "23: pred - 1, actual - 1\n",
      "24: pred - 2, actual - 2\n",
      "25: pred - 1, actual - 1\n",
      "26: pred - 1, actual - 1\n",
      "27: pred - 2, actual - 2\n",
      "28: pred - 2, actual - 2\n",
      "29: pred - 2, actual - 2\n",
      "30: pred - 1, actual - 2\n",
      "31: pred - 2, actual - 2\n",
      "32: pred - 1, actual - 1\n",
      "33: pred - 2, actual - 1\n",
      "34: pred - 1, actual - 1\n",
      "35: pred - 2, actual - 2\n",
      "36: pred - 2, actual - 2\n",
      "37: pred - 1, actual - 2\n",
      "38: pred - 2, actual - 2\n",
      "39: pred - 2, actual - 2\n",
      "40: pred - 2, actual - 1\n",
      "41: pred - 1, actual - 2\n",
      "42: pred - 1, actual - 1\n",
      "43: pred - 2, actual - 1\n",
      "44: pred - 2, actual - 2\n",
      "45: pred - 2, actual - 2\n",
      "46: pred - 1, actual - 1\n",
      "47: pred - 1, actual - 1\n",
      "48: pred - 1, actual - 1\n",
      "49: pred - 1, actual - 2\n",
      "50: pred - 2, actual - 2\n",
      "51: pred - 2, actual - 2\n",
      "52: pred - 2, actual - 1\n",
      "53: pred - 1, actual - 2\n",
      "54: pred - 2, actual - 2\n",
      "55: pred - 2, actual - 2\n",
      "56: pred - 2, actual - 1\n",
      "57: pred - 2, actual - 2\n",
      "58: pred - 2, actual - 2\n",
      "59: pred - 2, actual - 2\n",
      "60: pred - 2, actual - 2\n",
      "61: pred - 2, actual - 2\n",
      "62: pred - 2, actual - 1\n",
      "63: pred - 2, actual - 2\n",
      "64: pred - 2, actual - 2\n",
      "65: pred - 2, actual - 2\n",
      "66: pred - 1, actual - 2\n",
      "67: pred - 2, actual - 1\n",
      "68: pred - 2, actual - 1\n",
      "69: pred - 1, actual - 1\n",
      "70: pred - 1, actual - 1\n",
      "71: pred - 2, actual - 2\n",
      "72: pred - 2, actual - 2\n",
      "73: pred - 1, actual - 1\n",
      "74: pred - 1, actual - 1\n",
      "75: pred - 1, actual - 1\n",
      "76: pred - 1, actual - 1\n",
      "77: pred - 2, actual - 1\n",
      "78: pred - 2, actual - 2\n",
      "79: pred - 2, actual - 1\n",
      "80: pred - 2, actual - 2\n",
      "81: pred - 1, actual - 1\n",
      "82: pred - 1, actual - 1\n",
      "83: pred - 2, actual - 1\n",
      "84: pred - 2, actual - 1\n",
      "85: pred - 2, actual - 2\n",
      "86: pred - 2, actual - 2\n",
      "87: pred - 2, actual - 2\n",
      "88: pred - 1, actual - 1\n",
      "89: pred - 2, actual - 2\n",
      "90: pred - 2, actual - 2\n",
      "91: pred - 1, actual - 1\n",
      "92: pred - 1, actual - 1\n",
      "93: pred - 2, actual - 2\n",
      "94: pred - 2, actual - 2\n",
      "95: pred - 2, actual - 2\n",
      "96: pred - 2, actual - 2\n",
      "97: pred - 2, actual - 2\n",
      "98: pred - 2, actual - 1\n",
      "99: pred - 2, actual - 2\n",
      "100: pred - 2, actual - 2\n",
      "101: pred - 2, actual - 2\n",
      "102: pred - 2, actual - 2\n",
      "103: pred - 1, actual - 1\n",
      "104: pred - 2, actual - 2\n",
      "105: pred - 2, actual - 2\n",
      "106: pred - 2, actual - 1\n",
      "107: pred - 2, actual - 2\n",
      "108: pred - 2, actual - 1\n",
      "109: pred - 1, actual - 1\n",
      "110: pred - 1, actual - 1\n",
      "111: pred - 1, actual - 1\n",
      "112: pred - 1, actual - 1\n",
      "113: pred - 1, actual - 1\n",
      "114: pred - 2, actual - 1\n",
      "115: pred - 2, actual - 2\n",
      "116: pred - 2, actual - 2\n",
      "117: pred - 1, actual - 1\n",
      "118: pred - 2, actual - 2\n",
      "119: pred - 1, actual - 1\n",
      "120: pred - 2, actual - 2\n",
      "121: pred - 1, actual - 1\n",
      "122: pred - 2, actual - 2\n",
      "123: pred - 2, actual - 2\n",
      "124: pred - 2, actual - 2\n",
      "125: pred - 2, actual - 1\n",
      "126: pred - 2, actual - 2\n",
      "127: pred - 2, actual - 2\n",
      "128: pred - 1, actual - 1\n",
      "129: pred - 2, actual - 2\n",
      "130: pred - 2, actual - 1\n",
      "131: pred - 1, actual - 2\n",
      "132: pred - 1, actual - 1\n",
      "133: pred - 2, actual - 2\n",
      "134: pred - 2, actual - 2\n",
      "135: pred - 2, actual - 1\n",
      "136: pred - 2, actual - 2\n",
      "137: pred - 2, actual - 2\n",
      "138: pred - 1, actual - 1\n",
      "139: pred - 2, actual - 1\n",
      "140: pred - 2, actual - 1\n",
      "141: pred - 2, actual - 2\n",
      "142: pred - 1, actual - 1\n",
      "143: pred - 2, actual - 1\n",
      "144: pred - 2, actual - 2\n",
      "145: pred - 2, actual - 2\n",
      "146: pred - 2, actual - 2\n",
      "147: pred - 2, actual - 1\n",
      "148: pred - 2, actual - 2\n",
      "149: pred - 2, actual - 1\n",
      "150: pred - 2, actual - 2\n",
      "151: pred - 2, actual - 2\n",
      "152: pred - 2, actual - 1\n",
      "153: pred - 2, actual - 2\n",
      "154: pred - 2, actual - 1\n",
      "155: pred - 1, actual - 1\n",
      "156: pred - 2, actual - 2\n",
      "157: pred - 2, actual - 2\n",
      "158: pred - 2, actual - 1\n",
      "159: pred - 2, actual - 2\n",
      "160: pred - 1, actual - 1\n",
      "161: pred - 1, actual - 1\n",
      "162: pred - 1, actual - 2\n",
      "163: pred - 2, actual - 2\n",
      "164: pred - 2, actual - 2\n",
      "165: pred - 1, actual - 1\n",
      "166: pred - 2, actual - 2\n",
      "167: pred - 2, actual - 2\n",
      "168: pred - 2, actual - 2\n",
      "169: pred - 2, actual - 2\n",
      "170: pred - 2, actual - 2\n",
      "171: pred - 2, actual - 2\n",
      "172: pred - 1, actual - 2\n",
      "173: pred - 1, actual - 2\n",
      "174: pred - 2, actual - 1\n",
      "175: pred - 1, actual - 2\n",
      "176: pred - 1, actual - 2\n",
      "177: pred - 2, actual - 2\n",
      "178: pred - 2, actual - 2\n",
      "179: pred - 2, actual - 1\n",
      "180: pred - 1, actual - 2\n",
      "181: pred - 2, actual - 2\n",
      "182: pred - 2, actual - 1\n",
      "183: pred - 2, actual - 2\n",
      "184: pred - 2, actual - 2\n",
      "185: pred - 2, actual - 2\n",
      "186: pred - 2, actual - 2\n",
      "187: pred - 2, actual - 2\n",
      "188: pred - 2, actual - 1\n",
      "189: pred - 1, actual - 2\n",
      "190: pred - 2, actual - 1\n",
      "191: pred - 2, actual - 2\n",
      "192: pred - 1, actual - 1\n",
      "193: pred - 2, actual - 2\n",
      "194: pred - 2, actual - 1\n",
      "195: pred - 2, actual - 1\n",
      "196: pred - 2, actual - 2\n",
      "197: pred - 2, actual - 2\n",
      "198: pred - 2, actual - 2\n",
      "199: pred - 2, actual - 2\n",
      "200: pred - 2, actual - 2\n",
      "201: pred - 2, actual - 2\n",
      "202: pred - 2, actual - 1\n",
      "203: pred - 2, actual - 2\n",
      "204: pred - 1, actual - 1\n",
      "205: pred - 1, actual - 1\n",
      "206: pred - 2, actual - 2\n",
      "207: pred - 2, actual - 2\n",
      "208: pred - 2, actual - 2\n",
      "209: pred - 1, actual - 1\n",
      "210: pred - 2, actual - 1\n",
      "211: pred - 1, actual - 1\n",
      "212: pred - 2, actual - 2\n",
      "213: pred - 1, actual - 1\n",
      "214: pred - 2, actual - 1\n",
      "215: pred - 2, actual - 2\n",
      "216: pred - 2, actual - 2\n",
      "217: pred - 1, actual - 1\n",
      "218: pred - 2, actual - 2\n",
      "219: pred - 2, actual - 2\n",
      "220: pred - 1, actual - 1\n",
      "221: pred - 2, actual - 2\n",
      "222: pred - 2, actual - 2\n",
      "223: pred - 2, actual - 2\n",
      "224: pred - 2, actual - 1\n",
      "225: pred - 1, actual - 1\n",
      "226: pred - 1, actual - 1\n",
      "227: pred - 2, actual - 2\n",
      "228: pred - 1, actual - 1\n",
      "229: pred - 1, actual - 1\n",
      "230: pred - 2, actual - 2\n",
      "231: pred - 1, actual - 2\n",
      "232: pred - 2, actual - 2\n",
      "233: pred - 1, actual - 2\n",
      "234: pred - 2, actual - 2\n",
      "235: pred - 2, actual - 2\n",
      "236: pred - 2, actual - 2\n",
      "237: pred - 2, actual - 2\n",
      "238: pred - 2, actual - 1\n",
      "239: pred - 1, actual - 1\n",
      "240: pred - 2, actual - 2\n",
      "241: pred - 2, actual - 1\n",
      "242: pred - 2, actual - 1\n",
      "243: pred - 2, actual - 2\n",
      "244: pred - 1, actual - 2\n",
      "245: pred - 2, actual - 2\n",
      "246: pred - 2, actual - 1\n",
      "247: pred - 2, actual - 2\n",
      "248: pred - 1, actual - 1\n",
      "249: pred - 1, actual - 1\n",
      "250: pred - 2, actual - 1\n",
      "251: pred - 1, actual - 2\n",
      "252: pred - 1, actual - 1\n",
      "253: pred - 2, actual - 2\n",
      "254: pred - 2, actual - 2\n",
      "255: pred - 2, actual - 2\n",
      "256: pred - 2, actual - 2\n",
      "257: pred - 2, actual - 2\n",
      "258: pred - 2, actual - 1\n",
      "259: pred - 2, actual - 2\n",
      "260: pred - 2, actual - 1\n",
      "261: pred - 1, actual - 2\n",
      "262: pred - 2, actual - 2\n",
      "263: pred - 2, actual - 2\n",
      "264: pred - 1, actual - 1\n",
      "265: pred - 1, actual - 1\n",
      "266: pred - 2, actual - 1\n",
      "267: pred - 1, actual - 1\n",
      "268: pred - 1, actual - 1\n",
      "269: pred - 2, actual - 2\n",
      "270: pred - 2, actual - 1\n",
      "271: pred - 2, actual - 2\n",
      "272: pred - 2, actual - 2\n",
      "273: pred - 2, actual - 2\n",
      "274: pred - 1, actual - 1\n",
      "275: pred - 1, actual - 2\n",
      "276: pred - 2, actual - 2\n",
      "277: pred - 1, actual - 2\n",
      "278: pred - 1, actual - 1\n",
      "279: pred - 2, actual - 2\n",
      "280: pred - 1, actual - 1\n",
      "281: pred - 2, actual - 2\n",
      "282: pred - 2, actual - 2\n",
      "283: pred - 2, actual - 2\n",
      "284: pred - 1, actual - 1\n",
      "285: pred - 1, actual - 2\n",
      "286: pred - 2, actual - 1\n",
      "287: pred - 1, actual - 2\n",
      "288: pred - 2, actual - 1\n",
      "289: pred - 1, actual - 1\n",
      "290: pred - 2, actual - 2\n",
      "291: pred - 1, actual - 1\n",
      "292: pred - 2, actual - 2\n",
      "293: pred - 2, actual - 2\n",
      "294: pred - 2, actual - 1\n",
      "295: pred - 2, actual - 1\n",
      "296: pred - 1, actual - 2\n",
      "297: pred - 1, actual - 2\n",
      "298: pred - 2, actual - 2\n",
      "299: pred - 2, actual - 2\n",
      "300: pred - 1, actual - 1\n",
      "301: pred - 2, actual - 1\n",
      "302: pred - 2, actual - 2\n",
      "303: pred - 2, actual - 1\n",
      "304: pred - 2, actual - 1\n",
      "305: pred - 1, actual - 1\n",
      "306: pred - 2, actual - 1\n",
      "307: pred - 2, actual - 1\n",
      "308: pred - 1, actual - 1\n",
      "309: pred - 1, actual - 1\n",
      "310: pred - 2, actual - 1\n",
      "311: pred - 2, actual - 2\n",
      "312: pred - 2, actual - 2\n",
      "313: pred - 2, actual - 2\n",
      "314: pred - 1, actual - 2\n",
      "315: pred - 2, actual - 2\n",
      "316: pred - 2, actual - 1\n",
      "317: pred - 2, actual - 2\n",
      "318: pred - 1, actual - 1\n",
      "319: pred - 1, actual - 1\n",
      "320: pred - 1, actual - 1\n",
      "321: pred - 2, actual - 2\n",
      "322: pred - 1, actual - 1\n",
      "323: pred - 1, actual - 2\n",
      "324: pred - 2, actual - 2\n",
      "325: pred - 2, actual - 2\n",
      "326: pred - 2, actual - 2\n",
      "327: pred - 2, actual - 2\n",
      "328: pred - 2, actual - 1\n",
      "329: pred - 1, actual - 1\n",
      "330: pred - 1, actual - 1\n",
      "331: pred - 2, actual - 2\n",
      "332: pred - 1, actual - 1\n",
      "333: pred - 1, actual - 1\n",
      "334: pred - 2, actual - 2\n",
      "335: pred - 1, actual - 2\n",
      "336: pred - 2, actual - 1\n",
      "337: pred - 1, actual - 1\n",
      "338: pred - 1, actual - 2\n",
      "339: pred - 1, actual - 1\n",
      "340: pred - 2, actual - 2\n",
      "341: pred - 2, actual - 2\n",
      "342: pred - 2, actual - 1\n",
      "343: pred - 2, actual - 1\n",
      "344: pred - 2, actual - 2\n",
      "345: pred - 2, actual - 2\n",
      "done. \n"
     ]
    }
   ],
   "source": [
    "# leave-one-out classification\n",
    "\n",
    "predLabel = []\n",
    "trueLabel = []\n",
    "currSbj = 1;\n",
    "\n",
    "for sbj in subjectId:\n",
    "    # sbj is being tested, others are used to train the classifier\n",
    "    trainSbj = list(subjectId)\n",
    "    trainSbj.remove(sbj)\n",
    "    # create the dictionary\n",
    "    trainDict = make_dictionary(trainSbj, 5)\n",
    "    trainText = []\n",
    "    # create the training set\n",
    "    for tSbj in trainSbj:\n",
    "        tmpText = ''\n",
    "        for line in codecs.open(os.path.join(path, intvIdx, tSbj + '.txt'), 'r' , 'utf-8'):\n",
    "            tmpText = tmpText + ' ' + line\n",
    "        trainText.append([extract_feature(tmpText, trainDict), category[tSbj]])\n",
    "    # train classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(trainText)\n",
    "\n",
    "    # now the test set\n",
    "    trueLabel.append( category[sbj] )\n",
    "    testText = ''\n",
    "    for line in codecs.open(os.path.join(path, intvIdx, sbj + '.txt'), 'r' , 'utf-8'):\n",
    "        testText = testText + ' ' + line\n",
    "    # predict!\n",
    "    currPred = classifier.classify(extract_feature(testText, trainDict))\n",
    "    predLabel.append( currPred )\n",
    "    \n",
    "    print(str(currSbj) + ': pred - ' + str(currPred) + ', actual - ' + str(category[sbj]) )\n",
    "    currSbj = currSbj + 1\n",
    "\n",
    "print('done. ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.72      0.55      0.62       148\n",
      "          2       0.71      0.84      0.77       197\n",
      "\n",
      "avg / total       0.72      0.72      0.71       345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ok, it is POSSIBLE TO PREDICT cohort from the writings.\n",
    "\n",
    "print(metrics.classification_report(trueLabel, predLabel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  values = True                1 : 2      =     12.8 : 1.0\n",
      "              everything = True                1 : 2      =     12.8 : 1.0\n",
      "                   never = True                1 : 2      =      5.9 : 1.0\n",
      "                    into = True                1 : 2      =      5.8 : 1.0\n",
      "                     old = True                1 : 2      =      5.8 : 1.0\n",
      "                     now = True                1 : 2      =      5.6 : 1.0\n",
      "                     see = True                1 : 2      =      5.6 : 1.0\n",
      "                    dont = True                1 : 2      =      5.1 : 1.0\n",
      "                    such = True                1 : 2      =      4.9 : 1.0\n",
      "                    same = True                1 : 2      =      4.9 : 1.0\n",
      "                 writing = True                1 : 2      =      4.9 : 1.0\n",
      "                 trouble = True                1 : 2      =      4.9 : 1.0\n",
      "                     rap = True                1 : 2      =      4.9 : 1.0\n",
      "                   angry = True                1 : 2      =      4.9 : 1.0\n",
      "                    stay = True                1 : 2      =      4.9 : 1.0\n",
      "                    many = True                1 : 2      =      4.9 : 1.0\n",
      "                  happen = True                1 : 2      =      4.5 : 1.0\n",
      "               different = True                1 : 2      =      4.0 : 1.0\n",
      "                   those = True                1 : 2      =      4.0 : 1.0\n",
      "                   whole = True                1 : 2      =      4.0 : 1.0\n",
      "                 feeling = True                1 : 2      =      3.5 : 1.0\n",
      "                  others = True                1 : 2      =      3.5 : 1.0\n",
      "                   grade = True                2 : 1      =      3.3 : 1.0\n",
      "                   cause = True                1 : 2      =      3.2 : 1.0\n",
      "                    were = True                1 : 2      =      3.2 : 1.0\n",
      "                    take = True                1 : 2      =      3.1 : 1.0\n",
      "                  myself = True                1 : 2      =      3.0 : 1.0\n",
      "                     lot = True                1 : 2      =      3.0 : 1.0\n",
      "               religious = True                1 : 2      =      2.9 : 1.0\n",
      "                   smile = True                1 : 2      =      2.9 : 1.0\n",
      "                    than = True                1 : 2      =      2.9 : 1.0\n",
      "                   comes = True                1 : 2      =      2.9 : 1.0\n",
      "                   calms = True                1 : 2      =      2.9 : 1.0\n",
      "                   trust = True                1 : 2      =      2.9 : 1.0\n",
      "                   alone = True                1 : 2      =      2.9 : 1.0\n",
      "                everyday = True                1 : 2      =      2.9 : 1.0\n",
      "                 another = True                1 : 2      =      2.8 : 1.0\n",
      "               inportant = True                1 : 2      =      2.8 : 1.0\n",
      "                     has = True                1 : 2      =      2.8 : 1.0\n",
      "                    help = True                1 : 2      =      2.8 : 1.0\n",
      "                    even = True                1 : 2      =      2.8 : 1.0\n",
      "                anything = True                1 : 2      =      2.8 : 1.0\n",
      "                 without = True                1 : 2      =      2.8 : 1.0\n",
      "                    side = True                2 : 1      =      2.8 : 1.0\n",
      "                    part = True                2 : 1      =      2.8 : 1.0\n",
      "               sometimes = True                1 : 2      =      2.7 : 1.0\n",
      "                     but = True                1 : 2      =      2.6 : 1.0\n",
      "                  friend = True                1 : 2      =      2.6 : 1.0\n",
      "                    talk = True                1 : 2      =      2.5 : 1.0\n",
      "                     say = True                1 : 2      =      2.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# now, show me the informative features\n",
    "\n",
    "dictionary = make_dictionary(subjectId, 5)\n",
    "\n",
    "allText = []\n",
    "\n",
    "for sbj in subjectId:\n",
    "    tmpText = ''\n",
    "    for line in codecs.open(os.path.join(path, intvIdx, sbj + '.txt'), 'r' , 'utf-8'):\n",
    "        tmpText = tmpText + ' ' + line\n",
    "        allText.append([extract_feature(tmpText, dictionary), category[sbj]])\n",
    "\n",
    "# train classifier based on all information\n",
    "classifier = nltk.NaiveBayesClassifier.train(allText)\n",
    "\n",
    "# and show me the informative features\n",
    "classifier.show_most_informative_features(50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
